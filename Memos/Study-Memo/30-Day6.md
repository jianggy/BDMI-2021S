# Day6

李高远 No.30

#### AutoDiff 自动微分

###### 求导运算

1. 数值微分 Numerical Differentiation

   根据导数定义来计算：取极限

   舍入误差，计算效率低

2. 符号微分

   从表达式进行计算；需要保存大量中间变量

3. 自动微分

   - 前向模式

     计算图

     反向计算导数：输入x，y    z = f(x,y)

     只能计算输出对**某个**输入的微分

   - 前向模式的二元数（对偶数）方法Dual Number

   - 反向模式

     计算输出对所有输入的微分值

   - 反向模式的反向计算图方法

4. 自动微分与反向传播——本质一致性

   反向传播：在神经网络上执行梯度下降的主要算法

   可以认为反向传播是AD模式的一种

   更新网络权重，将网络评估转化为目标函数的优化，然后用AD的反向模式

5. 反向传播的问题

   需要保存中间的计算数值  ——   反向计算图方法

6. 静态和动态的反向计算图

   define and run  架构好模型然后输入数据 tf1

   define by run 训练时生成图  tf2 pytorch

#### 类与继承

1. super()：子类对象调用父类已经被覆盖的方法

   ```python
   class Cat:
   
       def __init__(self,x, y):
           self.x = x
           self.y = y
   
       def speak(self):
           print('123')
   
   class BlackCat(Cat):
   
       def __init__(self,x,y,z):
           Cat.__init__(self,x,y)
           self.z = z
   
       def speak(self):
           print('12324')
   
   c = BlackCat(1,2,3)
   c.speak()
   super(BlackCat,c).speak()
   ```

2. call方法

   调用实例名将直接调用call方法

   ```python
       def __call__(self, *args, **kwargs):
           print('123456')
   ```

#### 模块、层、模型

###### 层

```python
import tensorflow as tf
class Dense(tf.Module):

  def __init__(self, in_features, out_features, name=None):

    super().__init__(name=name)
    self.w = tf.Variable(
      tf.random.normal([in_features, out_features]), name='w')
    self.b = tf.Variable(tf.zeros([out_features]), name='b')

  def __call__(self, x):

    y = tf.matmul(x, self.w) + self.b
    return tf.nn.sigmoid(y)

dense_logistic = Dense(in_features=3, out_features=1)
dense_logistic.w = tf.constant([[-3.14],[-2.31],[2.16]])
x = tf.constant([[0.0288, -0.3256, 0.5925]])
y = dense_logistic(x)
print('Probability y=',y)
```

###### 模型

```python
import tensorflow as tf
class Dense(tf.Module):

  def __init__(self, in_features, out_features, name=None):

    super().__init__(name=name)
    self.w = tf.Variable(
      tf.random.normal([in_features, out_features]), name='w')
    self.b = tf.Variable(tf.zeros([out_features]), name='b')

  def __call__(self, x):

    y = tf.matmul(x, self.w) + self.b
    return tf.nn.sigmoid(y)


class SequentialModule(tf.Module):
  def __init__(self, name=None):
    super().__init__(name=name)

    self.dense_1 = Dense(in_features=3, out_features=3)
    self.dense_2 = Dense(in_features=3, out_features=2)

  def __call__(self, x):
    x = self.dense_1(x)
    return self.dense_2(x)

# You have made a model!
my_model = SequentialModule(name="the_model")

# Call it, with random results
print("Model results:", my_model(tf.constant([[2.0, 2.0, 2.0]])))

#子模型
print("Submodules:", my_model.submodules)

#模型内的所有变量
for var in my_model.variables:
  print(var, "\n")
```

###### 动态决定张量的维度

```python
class FlexibleDenseModule(tf.Module):
  # Note: No need for `in+features`
  def __init__(self, out_features, name=None):
    super().__init__(name=name)
    self.is_built = False
    self.out_features = out_features

  def __call__(self, x):
    # Create variables on first call.
    if not self.is_built:
      self.w = tf.Variable(
        tf.random.normal([x.shape[-1], self.out_features]), name='w')
      self.b = tf.Variable(tf.zeros([self.out_features]), name='b')
      self.is_built = True

    y = tf.matmul(x, self.w) + self.b
    return tf.nn.relu(y)
```

###### 模型的存储和恢复

```python
#存储
chkp_path = "my_checkpoint"
checkpoint = tf.train.Checkpoint(model=my_model)
checkpoint.write(chkp_path)
checkpoint.write(chkp_path)

#重新加载
new_model = MySequentialModule()
new_checkpoint = tf.train.Checkpoint(model=new_model)
new_checkpoint.restore("my_checkpoint")
```



#### 计算图

###### 图

nodes 节点表示变量或者操作符

edges 线表示计算间的依赖

灵活性、执行效率高、易于优化

###### 计算图的建立

```python
# Define a Python function.
def a_regular_function(x, y, b):
  x = tf.matmul(x, y)
  x = x + b
  return x

# `a_function_that_uses_a_graph` is a TensorFlow `Function`.
a_function_that_uses_a_graph = tf.function(a_regular_function)

# Make some tensors.
x1 = tf.constant([[1.0, 2.0]])
y1 = tf.constant([[2.0], [3.0]])
b1 = tf.constant(4.0)

orig_value = a_regular_function(x1, y1, b1).numpy()
# Call a `Function` like a Python function.
tf_function_value = a_function_that_uses_a_graph(x1, y1, b1).numpy()
assert(orig_value == tf_function_value)
```

递归的跟踪所调用的python函数

```python
def inner_function(x, y, b):
  x = tf.matmul(x, y)
  x = x + b
  return x

# Use the decorator to make `outer_function` a `Function`.
@tf.function
def outer_function(x):
  y = tf.constant([[2.0], [3.0]])
  b = tf.constant(4.0)

  return inner_function(x, y, b)

# Note that the callable will create a graph that
# includes `inner_function` as well as `outer_function`.
outer_function(tf.constant([[1.0, 2.0]])).numpy()
```

###### 计算图的加速

取决于运行的计算类型

多态函数：可以使用不同类型和形状的数据

#### 训练流程 Training Loops

1. 训练数据的获取

2. 定义模型

   用变量表示出权重和偏置

   使用模块封装变量并进行计算  ——   验证模型的有效性

3. 定义损失函数：度量输出与目标的匹配程度

4. 运行训练数据，从目标值计算损失

5. 计算损失的梯度，并使用优化器来调整变量以适应数据

6. 评估计算结果