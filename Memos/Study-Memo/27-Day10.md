# 第十周学习小结

2021.4.27  线下学习

 :smile:

## 1. 循环网络-回顾

+ U、V、W分别对应于输入到隐藏、隐藏到输出和隐藏到隐藏的全连接的权重矩阵
+ 循环网络的可变输出
  + 多个输入可以对应一个输出
  + 可变输入可以概括输入信息的情感
+ 输入层和输出层的格式一般应该是一样的
  + 判断矩阵的维度
+ 自然语言处理 onehot编码

## 2. LSTM深入

+ 下标t不一定表示时间，也可以 表示次序
+ Ux+Wh 简化为一个矩阵运算
+ 门的概念：对输入进行弱化，控制有多少可以通过。门对应着一个全连接矩阵
+ LSTM 增加了记忆单元的一条线
+ 记忆单元区别于隐藏层，上一部分遗忘后剩下的加上新输入的等于现在的
+ GRU简化了LSTM的状态和门，参数少、运行时间少，常用于语音识别

## 3. 循环网络—cell

+ RNN在Keras里面是内置的
+ 跨批次的状态性
+ 函数式API和model.add()两种方式
+ 词嵌入：把汉字或单词转化为向量
+ 默认情况下，RNN层的输出对应的是每个输入样本的一个向量，RNN层也可以返回对应每个输入的整个输出序列，如果设置return_sequences=True，输出形状为(batch_size, timesteps, units)
+ RNN类处理整批输入 序列，而RNN cell类仅处理单个时间步，有利于做研究
+ 允许输入是可变长度的，但是输出必须是固定长度的，便于后面的训练
+ 循环网络在处理序列相关数据的时候具有优势
+ 机器翻译 sequence to sequence：编码器-中间向量-解码器
+ 定制化输入的RNN：视频帧处理 、手写数字
+ 应用1：语音识别ASR
+ 应用2：图片注解

## 4. 单词嵌入向量

+ 用数字表示文本：独热编码、用一个的唯一的数字编码每个单词、单词嵌入向量
+ 优势：是高效、密集表示的方法，其中相似的单词具有相似的编码；无需手动指定此编码，嵌入向量是浮点值的密集向量，它们是可以训练的参数

***

感谢老师和助教们的耐心讲解和指导！:heartpulse:

