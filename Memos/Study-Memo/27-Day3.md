# 第三周学习小结

2021.3.9 线上学习

 :happy:

## 1. Numpy

+ numpy数组创建与类型定义
  + 使用array()函数创建数组
  + array接收list和tuple的混合，可以把list和tuple强行统一进来，都可以变成数组
+ numpy算数运算和矩阵运算
  + 可以进行数组和标量的算术运算
  + A*B 是对应位置相乘；使用点乘dot()函数可以按照线性代数的方式进行乘法运算
+ numpy数组形状变形 
  + reshape() 转换数组的形状
  + ravel() 将多维数组转换为一维数组
  + transpose() 转置

## 2.  深度学习1：人工神经元

+ 人工神经元——带权重的函数
  + 激活函数
    + Sigmoid函数（S形函数）（逻辑斯提回归函数）[0,1]
    + tanh函数 [-1,1]
    + ReLU函数  （处处连续但不是处处可导）
    + [代码实现](激活函数.py)
  + 逻辑斯提回归单元
  + 整流线性单元 （ReLU函数）
  + [人工神经元具体描述](人工神经元具体描述.py)
  + tips:要先把列表通过numpy转换为数组，才能进行接下来的运算;round 保留小数位数，按四舍五入的原则
+ 单个人工神经元的能力——布尔运算
  + [代码实现](AND.py)
+ 多个人工神经元的能力——二层网络解决XOR问题
  + 异或问题：不同的时候得到1，相同的情况得到0
+ [可以调用的激活函数](https://github.com/saturn-lab/BDMI-2021S/blob/main/Computing/Python3/numpy/Activation_Function_Introduction.ipynb)

## 3. 深度学习2：网络训练

+ 神经网络的应用
  + 神经网络方法：机器学习的监督学习方法
    + 根据训练数据集中的样本进行学习，然后推断新的实例
    + 首先需要有数据
  + 分类任务
  + 回归任务
+ 多层神经网络
  + Softmax处理，计算出一个概率分布向量，所有分量的和为1
  + 分对数 logit，把区间（0,1）内的数值，变换到R
  + logit和sigmoid互为反函数
+ 多层神经网络的训练
  + 确定损失函数
    + 损失：神经网络的输出和应该的输出之间的差异
    + 把标签和真实情况的差异度量化，这样就有了训练的目标
    + 回归任务的度量 均方差MSE
    + 分类任务的度量 交叉熵CE
    + [代码实现](损失函数.py)
  + 权重初始化
  + 反向传播
  + 权重更新

## 学习心得

通过理论学习和实践操作了解了人工神经元实现的底层逻辑，对深度学习的了解也更加深入，对于numpy 的应用熟练程度有待提高。

感谢老师和助教们的耐心讲解和指导！:heartpulse: