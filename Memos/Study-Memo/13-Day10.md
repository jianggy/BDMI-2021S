# 第十次课程小结

##### 王政和

##### 2017012269

##### 航天航空学院

***

1.$LSTM$深入

* 遗忘门：控制记忆单元的门
* 输入门：控制输入
* 记忆单元
* 输出门：控制隐藏状态
* 特点：相对$rnn$而言，参数量大，计算复杂

2.$GRU$

* 特点：参数少、运算简单、需要的训练数据少
* 更新门、重置门、节点状态、输出

3.循环网络的实现

* $rnn-cell$抽象：`output, state = cell(input_, state)`
* $Keras-RNN$类
* $Keras-RNNcell$类

4.跨批次的状态性（感觉大作业里面可以用）

5.单词嵌入向量