# Day12 Text & Audio

李高远 No.30

###### 循环网络文本生成

https://tensorflow.google.cn/tutorials/text/text_generation

###### transformer & BERT

https://tensorflow.google.cn/tutorials/text/transformer

- Pre-Training NLP

  Word2Vec 

- Encoder Decoder

- 获取输入句子的每一个单词的表示向量，表示成向量矩阵

- 矩阵传入Encoder，得到C

- 将C传入Decoder，Mask后依次翻译

  Mask：翻译一个单词时，只使用对应区域，遮掉其他部分

- Transformer结构：Attention机制

- Multi-Head Attention

- BERT 多任务模型  MLM、NSP

##### 自动语音识别 Automatic Speech Recognition

###### 语音识别的对象

Waveform、Frequency、Spectrogram

F函数、H参数

###### 词错误率WER

- 将标准答案和识别结果对齐
- 错误率等于替换、插入、删除的总数除以总长度
- 可能高于1

###### 经典的识别过程

语音信号傅里叶变化STFTT，连续语音分解为短期向量，变化到音素序列、字母序列、词汇序列

声学模型：GMM + HMM      语言模型 Bigram

###### 深度学习的引入

- DNN与经典方法的模块混合

  问题：模型比较复杂、需要复杂的训练方法、依赖于专业知识

- 纯DL——真正实用化

###### End to End 

- LVCSR 大词汇量连续语音识别 

###### 循环网络语音识别

- Google CLDNN 
- 连接主义的时间分类CTC
  - 挑战：输入语音X和输出文字Y之间没有确定的对齐方式
  - 原理：通过RNN形成概率分布矩阵，同时生成多个识别sequence，sequence折叠后有不同的额输出，计算概率最高的sequence
  - 实现：百度Warp-CTC

###### DeepSpeech

- 5个隐藏层，第4层为LSTM网络
- BatchNormalization  对每一层作归一化处理