# Day5

李高远 No.30

#### 复习：深度学习2

1. 基本名词

   iteration ----  epoch

2. 反向传播

   保存过多的中间变量

3. 自动微分AD  Automatic Differentiation

   计算图computation graph

   tensorflow2梯度带：自动构建反向计算图

#### Tensorflow2

1. 神经网络的一般流程

   1. 准备数据集
   2. 网络模型搭建
   3. 模型训练
   4. 模型应用 推断

2. 张量Tensor

   具有同一类型的多维数组

   python描述：3输入，隐藏层3神经元，输出层2神经元；输入[22,35,86]，权重矩阵用随机数填充

   ```python
   def softmax(x):
       e_x = np.exp(x - np.max(x))
       return e_x / e_x.sum()
   
   x = np.array([22,35,86])
   
   W1 = np.array(np.random.randn(3,3))
   W2 = np.array(np.random.randn(3,2))
   
   Y = softmax(np.dot(np.dot(x,W1),W2))
   print(Y)
   ```

#### 张量Tensor

1. 张量形状

   - 形状shape：每个维度的长度
   - 秩rank：维度数量
   - 轴axis / 维度dimension：特殊维度
   - 大小size：总项数

   所有张量是不可变的；无法更新其内容，只能创建新的张量

2. 创建

   ```python
   import tensorflow as tf
   import numpy as np
   #print(tf.__version__)
   
   rank_0_tensor = tf.constant(4)
   print(rank_0_tensor)
   #tf.Tensor(4, shape=(), dtype=int32)
   
   rank_1_tensor = tf.constant([2,3,4])
   print(rank_1_tensor)
   #tf.Tensor([2 3 4], shape=(3,), dtype=int32)
   
   rank_2_tensor = tf.constant([[1,2,3],[4,5,6],[7,8,9]],dtype=tf.float16)
   print(rank_2_tensor)
   #tf.Tensor(
   [[1. 2. 3.]
    [4. 5. 6.]
    [7. 8. 9.]], shape=(3, 3), dtype=float16)
   
   c = np.array(np.random.randn(3,3,3))
   rank_3_tensor = tf.constant(c)
   print(rank_3_tensor)
   #tf.Tensor(
   [[[-0.37193712 -0.52303026  1.77231451]
     [-0.46628677 -0.11936157 -1.67323613]
     [ 0.26632801  0.20263957  0.82986564]]
   
    [[ 0.46410109  0.52298603  1.84477156]
     [ 0.08396642  0.8224707   0.24705063]
     [-0.50286467  0.52554767 -0.04170012]]
   
    [[-0.36300453  0.11329752 -1.55938536]
     [-1.98464044 -0.39903145  0.9132259 ]
     [-1.54364896 -0.15409249 -0.84301793]]], shape=(3, 3, 3), dtype=float64)
   
   d = np.array(np.random.randn(3,3,3,3))
   rank_4_tensor = tf.constant(d)
   print(rank_4_tensor)
   #tf.Tensor(
   [[[[ 0.54870671 -1.92859478  0.21683383]
      [ 0.23486664  0.07904021  0.21811794]
      [ 0.57040685 -0.88222296  0.35519903]]
   
     [[ 2.2364277  -0.12478625 -0.45264948]
      [ 0.35035636  0.0143983   0.51698803]
      [ 1.20581686  0.39774969  0.70073844]]
   
     [[-0.36975957 -0.49516183  0.38924309]
      [-0.67498835 -0.30683952  0.1314772 ]
      [ 0.55483877  0.39189732 -0.69988832]]]
   
   
    [[[-1.38540642  0.62378874  0.66837038]
      [ 0.33014816 -0.23937912 -0.45245378]
      [ 1.80597005  0.28261629  0.0641862 ]]
   
     [[-0.61628561 -0.48437668 -0.43981741]
      [ 0.2041639   0.02701236  0.49111101]
      [ 0.174922   -2.24651036 -0.87992391]]
   
     [[-0.0431973  -0.54226578 -0.78435007]
      [ 0.1355913  -0.34138484  0.06674775]
      [ 0.06531992 -2.11722048  1.96350357]]]
   
   
    [[[ 0.65435776  0.52376129  0.9846779 ]
      [ 0.67802315 -0.30895332 -0.28600065]
      [-0.94764423 -1.128422   -1.09978051]]
   
     [[-0.28237886  0.66336754  0.06746082]
      [ 1.0646283  -0.64428246  1.79138747]
      [-0.40125982 -0.40114896 -0.20342492]]
   
     [[ 0.48763048 -0.43372942 -0.88786674]
      [ 1.24637034 -0.5326442   1.87518869]
      [ 0.1111871  -0.27142861 -0.31159776]]]], shape=(3, 3, 3, 3), dtype=float64)
   ```

   

3. tensor运算

   ```python
   #将张量转化为numpy数组
   np.array(rank_4_tensor)  #张量赋值给数组
   rank_4_tensor.numpy()  #调用张量中的numpy
   
   tf.add()  #对应元素相加
   tf.multiply()  #对应元素相乘
   tf.matmul()  #矩阵乘法
   
   tf.reduce_max()  #寻找元素最大值
   tf.argmax()  #求最大值的索引
   tf.nn.softmax()  #归一化
   
   tf.cast(x, dtype, name=None)  #张量数据类型转换，dtype为目标数据类型
   
   tf.reshape(tensor,shape,name=None) #shape为目标形状
   
   ```

4. Tensor 广播

   借用于Numpy中的等效功能

   运算时，为了适应大张量而对小张量进行扩展

5. Tensor索引

   ```python
   [::-1] #倒序
   [x, y]  #左闭右开
   [1:, :] #skip the first row
   ```

6. 特殊张量表示

   不规则张量

   ```
   tf.ragged.RaggedTensor()
   ```

   字符串张量 b''

   稀疏张量

   ```python
   sparse_tensor = tf.sparse.SparseTensor(indices=[[0,0],[0,0]],
                          values=[1,2],
                          dense_shape=[3,4])
   tf.sparse.to_dense(sparse_tensor)
   ```

   

#### Tensorflow变量

1. tf.variable

   ```python
   # 查看变量储存位置
   print(tf.debugging.set_log_device_placement)
   # 以张量方式查看
   tf.convert_to_tensor()
   # 查询最大值索引号
   tf.argmax()
   # 改变变量的值
   a.assign()
   a.assign_add() / a.assign_sub()  # 可以作为numpy数组运算（格式不同时）
   ```

   可以添加name，用于跟踪和调试

2. 梯度可以关闭   trainable = False

#### 自动微分

1. GradientTape

   ```python
   import tensorflow as tf
   
   x = tf.ones((2,2))
   
   with tf.GradientTape(persistent=True) as t:
       t.watch(x)
       y = tf.reduce_sum(x)
       z = tf.multiply(y, y)
   
   dz_dx = t.gradient(z,x)
   for i in [0,1]:
       for j in [0,1]:
           #assert dz_dx[i][j].numpy() == 8.0
           assert dz_dx[i][j].numpy() == 9.0
   
   dz_dy = t.gradient(z,y)
   assert dz_dy.numpy() == 8.0
   ```

2. test

   ```python
   def gradi(x):
   
       with tf.GradientTape() as tape:
           x = tf.Variable(x)
           y = x*x + tf.math.exp(x)
           print(tape.gradient(y, x).numpy())
   
   gradi(1.0)
   ```

3. 高阶导数