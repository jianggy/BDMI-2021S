# 第五周学习小结

2021.3.23 线下学习

 :happy:

## 1. Deep Learning II

+ 自动微分
  + 原理：创建一个反向的计算图

  + TensorFlow2梯度带：自动构建反向计算图

  + 用张量表示神经网络的运算

    ```python
    def Softmax(x):
        y = np.exp(x) / np.sum(np.exp(x))
        return y
    X=[22,35,86]
    W1=np.random.randn(3,3)
    W2=np.random.randn(3,2)
    N=np.dot(X,W1)
    Y=Softmax(np.dot(N,W2))
    print(Y)
    ```

## 2.  张量

+ 张量与变量的区别
  + 张量的实质是N维数组，所有的张量都是不可变的
  + 变量可以进行重新赋值
+ 张量的创建
  + tf.constant
+ 形状shape的定义
  + 是指张量的每个维度的长度
  + reshape：[3,-1] 行是3，列是把内存中其他的张开
+ Tensor运算
+ Tensor广播
  + 在一定条件下，对一组张量执行组合运算时，为了适应大张量，会对小张量进行“扩展”
+ Tensor索引
  + 从0开始，左闭右开
+ 特殊张量
  + 不规则张量
  + 字符串张量：注意字符串的长度不是张量的维度
  + 稀疏张量

# 3. 变量

+ 变量特性
  + 变量的值可以通过assign函数更改，但是变量一旦创建数据类型，变量形状将不可更改
  + 变量不能用assign改变数组，但是用numpy作为数组运算的话是可以的
+ 变量创建
+ 变量运算
+ 变量微分

## 4. 自动微分

+ 梯度带
  + TensorFlow为自动微分提供了tf.GradientTape API，根据某个函数的输入变量来计算它的导数
  + 通过创建一个持久的梯度带，可以计算同个函数的多个导数。这样在磁带对象被垃圾回收时，就可以多次调用‘gradient()’方法
+ 记录控制流
  + 由于磁带会记录所有执行的操作，Python控制流自然得到了处理
+ 高阶导数
  + 在‘GradientTape’上下文管理器中记录的操作会用于自动微分。如果导数实在上下文中计算的，导数的函数也会被记录下来。因此，同个API可以用于高阶导数。
+ 练习
  + 注意计算时要先转成TensorFlow里面的张量



***存在的问题***

TensorFlow的操作还不够熟练，主要是由于对于张量的表达形式不熟悉，需要多加练习。

***

感谢老师和助教们的耐心讲解和指导！:heartpulse: