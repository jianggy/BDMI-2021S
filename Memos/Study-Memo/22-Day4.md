### Memo for Week 4-李蔚然

#### What I've learned?

- Pandas
  - 一维数据结构Series、二维数据结构DataFrame、三维数据结构Panel
  - I/O操作读写文件

- 多层神经网络的训练

  - 损失函数
    	交叉熵或均方误差

  - 计算梯度：反向传播算法
        正向计算损失函数，反向计算损失函数对权重的梯度

  - 权重更新：随机梯度下降法
        找到一个函数的局部极小值，必须向函数上当前点对应的梯度的反方向，按照规定步长距离点，进行迭代搜索。

    ​	迭代公式如下：
    $$
    \theta=\theta-\eta\cdot\Delta_\theta J(\theta)
    $$
    ​	$\theta$为权重数值，$J(\theta)$为损失函数Loss，$\eta$为步长，又称学习率，学习率过大会引起震荡，学习率太小收敛太慢。

    ​	随机梯度下降法步骤如下：

    1. **随机**初始化每个神经元输入权重和偏差
    2. 选取一个**随机**样本
    3. 根据网络的输出结果，从最后一层开始，逐层计算每层权重的偏导数
    4. 逐层调整每层的权重，产生新的权重值
    5. 返回步骤2，继续**随机**选取下一个样本

    ​    随机梯度下降法的核心是一个“随机样本”

- 网络训练实例：猜测性别

- 张量

- Tensorflow