# Day3

李高远 No.30

### Numpy

1. 创建数组和数据类型定义

   包容性

   ```python
   np.array
   a = np.array([[1,2,3],[1,2,3]])
   b = np.array(([1,2,3],[1,2,3]))
   ```

2. 算数运算

   对应位置

   ```python
   +， - ， *
   c = np.arange(4)
   array([0, 1, 2, 3])
   c+4
   array([4, 5, 6, 7])
   c*4
   array([ 0,  4,  8, 12])
   ```

   函数算数运算符

   ```python
   np.sin()
   np.sqrt()
   np.square()
   ```

3. 矩阵乘积

   ```python
   np.dot(A, B) #点乘
   A * B
   ```

4. 数组变形

   ```python
   np.reshape()  #转化为特定的形状
   np.ravel()  #转化为一维数组
   np.transpose()  #转置
   ```

5. Numpy使用

   ```python
   np.matmul()  #创建一个单位矩阵
   np.zeros()  #创建零矩阵
   c = np.arange(Start, Stop, Step)
   np.identity()  #矩阵相乘
   np.vstack #垂直叠加
   ```

6. Numpy调试

   ```python
   array.shape #返回形状
   array.dtype #返回数据类型datatype
   ```

#### 深度学习1：人工神经元

1. 激活函数

   - sigmoid   0 ~ 1

   - tanh  -1 ~ 1

   - ReLU max(x, 0)

     ```python
     import matplotlib.pyplot as plt
     import numpy as np
     
     def sigmoid(x):
         a = 1/(1 + np.exp(-x))
         return a
     
     def tanh(x):
         a = (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))
         return a
     
     def ReLU(x):
         a = np.maximum(x,0)
         return a
     
     x = np.linspace(-4, 4, 200)
     
     plt.plot(x, sigmoid(x))
     plt.plot(x, tanh(x))
     plt.plot(x, ReLU(x))
     plt.show()
     ```

     模拟神经元的触发激活特性

     整流线性单元 F（w, x） 调节权重w

2. 布尔运算

   - 与AND   或OR   非NOT与非NAND    异或XOR
   - e.g. 描述一下AND门

   ```python
   w = [20,20]
   
   def AND(x):
       a = 1 / (1 + np.exp(-(np.dot(w,x)-30)))
       print(a)
       return 0
   
   AND([1,1])
   AND([0,1])
   AND([1,0])
   AND([0,0])
   
   0.9999546021312976
   4.5397868702434395e-05
   4.5397868702434395e-05
   9.357622968839299e-14
   ```

   偏置+权重的选择

   - XOR的实现

     实现1：两层网络： {X1 OR X2} AND {NAND{X1,X2}}

     实现2：{X1 OR X2} AND{ (NOT X1) OR (NOT X2)}

#### 深度学习2：人工神经元

1. 多层神经网络

   - 监督学习
   - 分类  classification  回归 regression

2. 单个人工神经元

   - 分类任务

3. 更多层数的深度神经网络

   - softmax 计算概率分布向量
   - logit 分对数  sigmoid 反函数

4. 多层神经网络——实现自动化更新  模型的训练

   - 确定损失函数 Loss 

     损失：输出值与标签值的差异

     引入度量函数——表示这种差异

     回归任务：均方误差MSE

     ​        MSE：误差平方和的平均数

     ```python
     def MSE(x,y):
         m = 0
         for i in x:
             m += pow((i-y),2)
         a = x.shape
         m = m / a[0]
         return m
     
     print(MSE(aaa,80))
     ```

     分类任务：交叉熵CE

     ​         CE：负对数似然损失函数

     ```python
     def CE(x,y):
         a = -np.dot(x,np.log(y))
         return a
     
     print(CE([1,0],[0.9,0.1]))
     print(CE([0,1],[0.2,0.8]))
     ```

     训练过程转化为损失函数的最小化过程

   - 权重初始化 Initialization

   - 反向传播 Backward Propagation

   - 权重修正 Weights Adjusting 