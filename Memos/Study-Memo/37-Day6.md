### Week 6 summary

2021.3.30

***

#### tensorflow2

- 自动微分

  - 数值

    求极限，不准确

  - 符号

    python中有一些符号演算的库；进行对于表达式的运算；效率低

  - AD automatic differentiation

    - 历史：控制论
    - 前向模式；基于dual number 的实现(超复数）见综述3.1.1；求出jacobian matrix 的一列只有一个输入的微分
    - 反向模式；求得[jacobian matrix]^T^的一列；因为输入多输出少所以很实用

  - 反向传播与AD反向模式是交织发展，~~可以混淆~~
  - 通过计算图减少对于内存的消耗
  - 图论，反拓扑序balabala

- python类与继承

  - ```python
    class cat:
        i = 111
        def __init__(self,name,age):
            cat.name=name
            cat.age=age
        def speak(self):
            print('hello,i am a cat')
        def __call__(self):
            print('hello,i am '+cat.name)
    class blackcat(cat):
        def __init__(self,name,age):
            cat.__init__(self,name,age)#如果没有新的属性可以不写构函，会自动调用
        def speak(self):
            print('hello,i am a blackcat')
    
    c=blackcat('ab',2)
    c()
    #__call__()方法特殊：c.__call__()=c()
    c.speak()
    super(blackcat,c).speak()#super(,)超类，父类
    ```

- 模块、层、模型

  - 模块module

    一个神经元，一次线性操作

  - 层dense

    ```python
    class Dense(tf.Module):
        def __init__(self,w,name=None):
            super().__init__(name=name)
            self.w=tf.constant(w)
        def __call__(self,x):
            y=tf.matmul(x,self.w)
            print(tf.nn.sigmoid(y).numpy())
    
    c=Dense(w=[[-3.14],[-2.31],[2.16]])
    x=tf.constant([[0.0288],[-0.3256],[0.5925]])
    x=tf.transpose(x)#记得转置
    c(x)
    #不能少[]x与w
  #or
    c=Dense(w=[[-3.14],[-2.31],[2.16]])
    x=tf.constant([[0.0288,-0.3256,0.5925]])
    ```
    
    张量控制数据作为多个神经元
    
  - 模型model

    ```python
    class sequentialmodule(tf.Module):
        def __init__(self,name=None):
            super().__init__(name=name)
            self.dense_1 = Dense(in_features=,out_features=)
            self.dense_2 = Dense(in_features=,out_features=)
            #与dense的架构有关
        def __call__(self,x):
            x = self.dense_1(x)
            return self.dense_2(x)
    #串联两个层
    ```

  - 动态决定张量维度

    加一个判断是否已经built，然后创建随机权重w，利用x.shape[-1]找到x的行数

  - 模型存贮和恢复

    ```python
    chkp_path = "my_checkpoint"
    checkpoint = tf.train.Checkpoint(model=my_model)#实例化
    checkpoint.write(chkp_path)#存储文件，命名为chkp_path
    tf.train.list_variables(chkp_path)#打印储存的变量
    checkpoint.restore("my_checkpoint")#retore
    ```

    data数据本身与index索引文件

  - tensorboard可视化

  - 存储整个模型、恢复整个模型

- 计算图：使用动态计算图，友好一点

  - 装饰器,改为一个tf函数（建立计算图）

    @tf.function

  - %load_ext tensorboard

    #启用插件才可以看到图

  - 利用tensorboard显示图

  - 有加速性能

- 训练流程train loop

  - 使用keras模型

:kissing_heart: