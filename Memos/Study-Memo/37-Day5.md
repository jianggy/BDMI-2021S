### Week 5 summary

2021.3.23

***

深度学习Tensor flow实践

Tensor flow 2基础

- 反向传播
  
- 自动微分原理：创建一个反向微分图；tensorflow梯度带
  
- 张量
  
  N维数组
  
  - 用numpy创建张量
  
  ```python
  np.random.normal(size=(2,2,2))
  ```
  
  - 张量表示神经网络
```
  softmax = labmda x:...
  x= np.array([1,2,3])
  w1=np.random.random((3,3))
  w2=np.random.random((3,2))
  n=np.dot(x,w1)
  y=softmax(np.dot(n,w2))
```

- 张量的创建with tensorflow
  
  ![img](https://qn-st0.yuketang.cn/FiNl7Wri3_FkfiT5R1wuneryKNKL)
  
```python
import tensorflow as tf 
r3 = tf.constant(np.random.randn(3,3,3,3))
r3 = tf.constant(np.ones(shape=(3,3,3,3)))
r = tf.constant(4)#div=0
r = tf.constant([4])#div=1
r = tf.constant([[4]])#div=2...
```

- 常数（不能更改）运算

```python
tf.add() a+b
tf.multiply() a*b
tf.matmul() a@b
tf.reduce_max()#寻找元素最大值
tf.argmax()#求最大值的索引
tf.nn.softmax()#对元素进行归一化处理
```

- 与numpy的转换

```python
a = tf.constant(6)
a.numpy()#方法调用
b = np.array(a)#assigned for ever
```

- 关于dtype

```python
tf.cast(a,dtype=tf.int32)#将a数据类型转换为int32
#dtype=(3,)imply that the second dimension has only one number
#dtype=(3,none)imply that it's a ragged tensor
```

- 关于tensor reshape
  
  ```PYTHON
  #tf.Tensor(x,shape=(3,2,5))
  tf.reshape(x,[3*5,2])
  tf.reshape(x,[3,-1])
  tf.reshape(x,[3,2,5])#不是直接转置按顺序变
  ```
  
- tensor广播：运算时把小张量扩展到与大张量相匹配；

  application：save the memory

  ```PYTHON
  x = tf.constant([1,2,3])
  x = tf.reshape(x,[3,1])
  y = tf.range(1,5)
  print(x,y)
  print(tf.multiply(x,y))
  ```

- 索引与numpy相似,同样可以用来切片
  
- 特殊张量
  
  - ragged tensor
  
    ```python
    tf.ragged.constant()
    ```
  
  - string 
  
    string is a atom element, so 

    ```python
    tf.contant([’laskdhf‘])#shape=(1,),dtype=string
    #split the string tensor
    #it will be splitted into a ragged tensor according to the blank
    tf.strings.split(x)
    #tranform string into byte
    y=tf.strings.bytes_split(x)
    z=tf.io.decode_raw(x,uint8)
    ```
  
  - 稀疏张量sparse tensor
  
    ```python
    x = tf.sparse.SparseTensor(indices=[[],[]],
                          valuces=[,],
                          dense_shape=[,])
    y = tf.sparse.to_dense(x)#convert to dense
    ```
  
    
  
- 变量

  创建、赋初值、类型；结构、运算；改值

  ```python
  my_tensor = tf.constant([[1,2],[3,4]])
  my_variable = tf.Variable(my_tensor)
  #the type is variable，but then unchangeable
  
  print(my_variable.shape)
  print(my_variable.dtype)
  print(my_variable.numpy)
  
  print(tf.convert_to_tensor(my_variable))
  print(tf.argmax(my_variable.shape))#find the index of the max argument
  #the variable is unchangeable, so if you want to reshape it, you will get a tensor created
  print(tf.reshape(my_variable.shape,([1,4])))
  
  my_variable.assign(这里全部重写)
  或者
  my_variable.assign_add(写一个结构相同的array)
  
  命名
  tf.variable(x,name = )
  关闭梯度
  tf.variable(x,trainable = False)
  ```

  

- 自动微分
  
  ```python
  import tensorflow as tf 
  x= tf.ones((2,2))
  with tf.GradientTape() as t:
      t.watch(x)
      y = tf.reduce_sum(x)
      z = tf.multiply(y,y)
  dy_dx=t.gradient(y,x)
  dz_dy=t.gradient(z,y)#with 只能用一次梯度方法

  建立持久的梯度带，可以多次使用梯度方法
  with tf.GradientTape(persistent=True) as t:
      ...
  del t
  
  会记录控制流可以计算高阶导数（两次with即可）
  ```
  
- what is A P I?

- the grammar of 'with'

  

#### 杂记

***

1. tensorflow运行时报错很多

   ```python
   import os
   os.environ["CUDA_VISIBLE_DEVICES"] = "-1"
   #不适用GPU计算
   os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"
   '''设置日志等级
   取值 0 ： 0也是默认值，输出所有信息
   取值 1 ： 屏蔽通知信息
   取值 2 ： 屏蔽通知信息和警告信息
   取值 3 ： 屏蔽通知信息、警告信息和报错信息
   '''
   import tensorflow as tf
   多行引用'''''' OR 选中要引用区域ctrl+/
   ```
   
2. np.random.randn(2,2,2,2)    

   =   np.random.standard_normal(size=(2,2,2,2))调用方式不同

3. 异常语法

   - try：

     except Exception as e:

     except ValueError as a:

     final:

   - 异常传递

   - 主动抛出异常

     raise

   - 断言

     assert   =  

     false则返回异常

   ### 问题

   ***

   操作比较少，不太熟

   :kissing_heart:
