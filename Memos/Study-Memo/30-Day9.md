# Day9 RNN

李高远 No.30

#### TensorFlow2 数据流水线

###### tf.data 

- 处理大批量的数据，从不同的文件格式中读取

- ```python
  tf.data.Dataset.from_tensor_slices
  tf.data.Dataset.from_generator #减少内存开销
  tf.data.TFRecordDataset 
  tf.data.TextLineDataset #纯文本
  tf.keras.preprocessing.image.ImageDataGenerator #读取图片
  tf.data.experimental.make_csv_dataset #读取csv
  ```

  ```python
  dataset.shuffle(buffer_size=100) #打乱数据
  dataset.map(预处理函数)
  ```

  示例1：读取csv

  ```python
  import tensorflow as tf
  import pandas as pd
  
  def data():
      data_file = './data.csv'
      df = pd.read_csv(data_file)
      data_batches = tf.data.experimental.make_csv_dataset(
          data_file,batch_size=4,label_name='Q2'
      )
      for feature_batch, label_batch in data_batches.take(1):
          print(feature_batch)
          print(label_batch)
  data()
  ```

#### Keras函数式API

###### 函数式API

- 输入和输出的变换
- model.summary()
- keras.utils.plot_model(model, '***.png', show_shapes = True)

```python
encoder_input = keras.Input(shape=(28, 28, 1), name="img")
x = layers.Conv2D(16, 3, activation="relu")(encoder_input)
x = layers.Conv2D(32, 3, activation="relu")(x)
x = layers.MaxPooling2D(3)(x)
x = layers.Conv2D(32, 3, activation="relu")(x)
x = layers.Conv2D(16, 3, activation="relu")(x)
encoder_output = layers.GlobalMaxPooling2D()(x)

encoder = keras.Model(encoder_input, encoder_output, name="encoder")
encoder.summary()

x = layers.Reshape((4, 4, 1))(encoder_output)
x = layers.Conv2DTranspose(16, 3, activation="relu")(x)
x = layers.Conv2DTranspose(32, 3, activation="relu")(x)
x = layers.UpSampling2D(3)(x)
x = layers.Conv2DTranspose(16, 3, activation="relu")(x)
decoder_output = layers.Conv2DTranspose(1, 3, activation="relu")(x)

autoencoder = keras.Model(encoder_input, decoder_output, name="autoencoder")
autoencoder.summary()
```

###### 复杂计算图模型和共享层

- 具有多个输入和多个输出
- 小ResNet模型：非线性链接拓扑

#### RNN Recurrent Neural Network

###### 基本结构

- 时间维度上每一个相同的权重 weights sharing
- 循环的反馈连接
- vanilla RNN

###### 损失函数

- 训练损失：将输入训练映射到对应的输出序列
- 计算softmax(o)，与目标y比较从而得到损失L

###### 训练：时间反向传播BPTT

- 一次前向  +  一次反向   复杂度为O(T)
- 训练代价很大，无法并行化
- 梯度截断 Gradient Clipping 

###### 双向RNN Bidirectional RNN

- 一个时间上从开始 + 另一个时间从末尾开始

###### Gate 门结构

- Input 和 Control 是形状一致的张量
- Control 经过 Sigmoid 得到一个0-1的张量
- Input与sigma(Control)相乘
- 可以反向传播训练

###### LSTM 长短时记忆网络

- 三个辅助门单元 ： 控制输入输出和储存
- 输入门、遗忘门、记忆单元、输出门
- 记忆单元：LSTM的核心  对时间的积分

##### GRU 简化LSTM

###### 更新门与重置门

- Update Gate：输入+遗忘   控制历史信息对于当前隐藏层输出的影响
- Reset Gate：重置门关闭则忽略历史信息

##### 课程大作业说明

- 时频谱图数据：40个训练 20个测试
- classification 
- accuracy
- AudioNet
- 提交：NoteBook 给定测试集及其实验结果  实验报告  