# Day10 RNN(continued)

李高远 No.30

#### 循环网络

###### 输入与可变输出

- 可变输出：在时间上展开，最终单个输出
- 计算方程

#### LSTM

###### Gate 门

- Input 和 Control 是形状一致的张量
- Control 经过 Sigmoid 得到一个0-1的张量
- Input与sigma(Control)相乘

###### Cell状态

- 遗忘后上一时刻的结果 + 新输入
- 线性加权获得新的记忆单元状态  

###### 控制输出

- 控制隐藏状态
- 全连接矩阵

###### 小结

- 4个全连接矩阵作为权重
- 参数量大，计算复杂

#### GRU

###### Gated Recurrent Unit

- 参数更少、运算更快、需要的训练数据少
- 大数据量时，LSTM 表达能力强

#### RNN 的keras实现

###### RNN-Cell抽象

- O(output ) + new_state )<= I(input) + S(state)
- hidden units : 模型的容量大小
- state : 隐含了之前所有的输出信息

###### Keras RNN

```python
keras.layers.RNN
keras.layers.SimpleRNN
keras.layers.LSTM
keras.layers.GRU
```

- 如果设置return_sequences =True，输出形状为 batch_size , timesteps

###### RNN Cell

```python
keras.layers.SimpleRNNCell # SimpleRNN 层
keras.layers.LSTMCell # LSTM 层
keras.layers.GRUCell # GRU 层
```

###### 跨批次的状态性

- 处理长序列时进行有效的切分
- 双向RNN

###### 定制化输入的RNN

- 在单个时间步上输入更多的信息

#### Sequence to Sequence Model

###### 多层LSTM

- 1 个 Deep LSTM 将输入序列映射为一个固定维度的中间向量
- 1 个 Deep LSTM 将中间向量解码为目标序列
- Sequence output prediction
- attention mechanism

###### BERT：BERT: Pre training of Deep Bidirectional Transformers for Language Understanding

###### 神经图灵机 Neural Turing Machines

#### 单词嵌入向量 TensorFlow2

###### One-hot encodings

- 创建长度等于词汇量的零向量表，对应位置放1
- 问题：十分稀疏 sparse

###### 用唯一的数字编码每个单词

- 句子编码为密集向量
- 问题：不会捕获单词之间的任何关系

###### 单词嵌入向量

- 相似的单词拥有相似的编码
- 嵌入向量是可训练的
- 常见于8维，大型数据集可能有1024维

