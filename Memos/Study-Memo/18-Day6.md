### BDMI --第六周学习小结

##### 赵子瑜

本周课程的主要内容有：

1. 自动微分的原理

   > 反向传播是自动微分的一种
   >
   > 反向传播时需要保留一些中间数值用于导数的计算，会占用一定量的内存
   >
   > 静态/动态的反向计算图

2. tensorflow 

   > 所有的模型和层都是tf.Module的子类
   >
   > 在继承tf.module时需要实现如下函数：
   >
   > ```
   > def__call__(self, x):
   > 	return...
   > ```
   >
   > 查看变量：
   >
   > ```
   > for var in my_model.variables:
   > 	print(var, "\n")
   > ```
   >
   > 动态决定张量维度：
   >
   > ```
   > def__init__(self, out_features, name=None):
   > 	super().__init__(name=name)
   > 	self.is_built = False
   > 	...
   > def__call__(self, x):
   > 	if not self.is_built:
   > 		self.w = tf.Variable(
   > 		tf.random.normal([x.shape[-1], self.out_features]), name='w')
   > 		self.b = tf.varivable(tf.zeros(self.out_features), name='b')
   > 		self.is_built = True
   > ```
   >
   > 模型的储存：
   >
   > ```
   > chnk_path = 'my_checkpoint'
   > checkpoint = tf.train.Checkpoint(model=my_model)
   > checkpoint.write(chnk_path)
   > ```
   >
   > 

3. 训练模型

   > 获取训练数据
   >
   > 定义模型
   >
   > 定义损失函数
>
   > 定义optimizer
   >
   > train
   
   
